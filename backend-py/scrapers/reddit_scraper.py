"""
Reddit Scraper Module
Scrapes Reddit threads and comments using Reddit JSON API (fallback to Selenium)
"""
import time
import random
import re
import requests
import json
from datetime import datetime
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse, quote_plus
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException, TimeoutException


class RedditScraper:
    """Scraper for Reddit threads and comments"""
    
    def __init__(self):
        self.driver = None
        
    def setup_driver(self):
        """Setup undetected Chrome driver"""
        if self.driver:
            try:
                # Test if driver is still alive
                _ = self.driver.current_url
                return
            except:
                try:
                    self.driver.quit()
                except:
                    pass
                self.driver = None
            
        print("Setting up undetected-chromedriver...")
        
        try:
            # Create completely fresh options object each time
            options = uc.ChromeOptions()
            options.add_argument("--headless=new")
            options.add_argument("--start-maximized")
            options.add_argument("--disable-blink-features=AutomationControlled")
            options.add_argument("--disable-dev-shm-usage")
            options.add_argument("--disable-gpu")
            options.add_argument("--window-size=1920,1080")
            
            # Add more realistic browser settings
            options.add_argument("--disable-web-security")
            options.add_argument("--allow-running-insecure-content")
            options.add_argument("--lang=en-US")
            options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36")
            
            # Disable automation indicators
            prefs = {
                "credentials_enable_service": False,
                "profile.password_manager_enabled": False,
                "profile.default_content_setting_values.notifications": 2
            }
            options.add_experimental_option("prefs", prefs)
            options.add_experimental_option("excludeSwitches", ["enable-automation"])
            options.add_experimental_option("useAutomationExtension", False)
            
            # Use version 144 explicitly to match current Chrome
            self.driver = uc.Chrome(
                options=options,
                version_main=144,
                use_subprocess=False
            )
            print("‚úì Undetected Chrome initialized")
        except Exception as e:
            print(f"‚ö† Failed to setup Chrome with version 144, trying auto-detect: {e}")
            try:
                # Fallback: let it auto-detect
                options = uc.ChromeOptions()
                options.add_argument("--headless=new")
                options.add_argument("--no-sandbox")
                options.add_argument("--disable-dev-shm-usage")
                self.driver = uc.Chrome(options=options, use_subprocess=False)
                print("‚úì Undetected Chrome initialized (auto-detected version)")
            except Exception as e2:
                print(f"‚úó Failed to setup Chrome: {e2}")
                raise
        
    def close_driver(self):
        """Close the browser driver"""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None
    
    def dismiss_modals(self):
        """Dismiss Reddit modals/popups"""
        try:
            time.sleep(2)
            self.driver.execute_script("""
                document.querySelectorAll('[role="dialog"]').forEach(el => el.remove());
                document.querySelectorAll('.Overlay').forEach(el => el.remove());
            """)
            close_buttons = self.driver.find_elements(By.CSS_SELECTOR, 
                "button[aria-label='Close'], button[icon='close']")
            for btn in close_buttons:
                try:
                    if btn.is_displayed():
                        btn.click()
                        time.sleep(1)
                except:
                    pass
            print("‚úì Dismissed modals")
        except:
            pass
    
    def scroll_page(self, scrolls: int = 8):
        """Scroll page to load more content"""
        print(f"üìú Scrolling to load comments ({scrolls} scrolls)...")
        for i in range(scrolls):
            self.driver.execute_script("window.scrollBy(0, 800);")
            time.sleep(random.uniform(0.5, 1.5))
        print("‚úì Scrolling complete")
    
    def scrape_reddit_thread(self, url: str) -> Dict[str, Any]:
        """
        Scrape a Reddit thread including post and comments
        
        Args:
            url: Reddit thread URL
            
        Returns:
            Dictionary containing thread data and comments
        """
        try:
            self.setup_driver()
            
            print(f"\nüåê Loading Reddit: {url}")
            self.driver.get(url)
            time.sleep(random.uniform(5, 8))
            
            self.dismiss_modals()
            self.scroll_page()
            
            # Extract post data
            post_data = self.driver.execute_script("""
                let post = {};
                
                // Title
                let titleEl = document.querySelector('[slot="title"]') || 
                              document.querySelector('h1');
                post.title = titleEl ? titleEl.textContent.trim() : '';
                
                // Author
                let authorEl = document.querySelector('[slot="authorName"]') || 
                               document.querySelector('[data-testid="post-author"]');
                post.author = authorEl ? authorEl.textContent.trim() : '';
                
                // Post text
                let textEl = document.querySelector('[slot="text-body"]') ||
                            document.querySelector('[data-click-id="text"]');
                post.text = textEl ? textEl.textContent.trim() : '';
                
                // Subreddit
                let subredditEl = document.querySelector('[data-testid="subreddit-name"]');
                post.subreddit = subredditEl ? subredditEl.textContent.trim() : '';
                
                return post;
            """)
            
            print(f"‚úì Post: {post_data['title'][:60]}...")
            
            # Extract comments
            comments_data = self.driver.execute_script("""
                let comments = [];
                let commentElements = document.querySelectorAll('[data-testid="comment"]');
                
                commentElements.forEach(el => {
                    let author = el.querySelector('[data-testid="comment_author_link"]')?.textContent.trim() || '';
                    let text = el.querySelector('[data-testid="comment"]')?.textContent.trim() || '';
                    
                    if (text.length > 20) {
                        comments.push({
                            author: author,
                            text: text
                        });
                    }
                });
                
                return comments;
            """)
            
            print(f"‚úì Extracted {len(comments_data)} comments")
            
            return {
                'url': url,
                'post': post_data,
                'comments': comments_data,
                'scrape_timestamp': datetime.now().isoformat()
            }
            
        finally:
            self.close_driver()
    
    def search_using_json_api(
        self,
        subreddit: str,
        query: str,
        limit: int = 5,
        before_date: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Search Reddit using JSON API (no selenium needed)
        Returns full signal data including post content
        
        Args:
            subreddit: Subreddit to search
            query: Search query
            limit: Maximum number of results
            before_date: Filter posts before this date (YYYY-MM-DD)
        """
        try:
            # Build Reddit JSON API URL
            search_url = f"https://www.reddit.com/r/{subreddit}/search.json"
            params = {
                'q': query,
                'restrict_sr': 'on',
                'sort': 'relevance',
                't': 'all',
                'limit': limit
            }
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36'
            }
            
            print(f"üîç Searching Reddit JSON API: {search_url}?q={query}")
            
            # Retry logic for connection issues
            for attempt in range(3):
                try:
                    response = requests.get(search_url, params=params, headers=headers, timeout=15)
                    response.raise_for_status()
                    break
                except requests.ConnectionError as e:
                    error_msg = str(e)
                    if "10061" in error_msg or "actively refused" in error_msg.lower():
                        print(f"‚ùå Reddit is blocking the connection!")
                        print(f"   This is Reddit's anti-bot protection.")
                        print(f"   Solutions:")
                        print(f"   1. Use a VPN to change your IP address")
                        print(f"   2. Try from a different network (mobile hotspot)")
                        print(f"   3. Use Reddit's official API (see REDDIT_TROUBLESHOOTING.md)")
                        print(f"   4. Skip Reddit scraping (use only Financial/News scrapers)")
                        raise ConnectionError(
                            "Reddit blocked the connection. "
                            "Try using a VPN or see REDDIT_TROUBLESHOOTING.md for solutions."
                        ) from e
                    elif attempt < 2:
                        wait_time = (attempt + 1) * 2
                        print(f"‚ö†Ô∏è Connection failed (attempt {attempt + 1}/3), retrying in {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        raise
                except requests.Timeout as e:
                    if attempt < 2:
                        wait_time = (attempt + 1) * 2
                        print(f"‚ö†Ô∏è Request timeout (attempt {attempt + 1}/3), retrying in {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        raise
            
            data = response.json()
            posts = data.get('data', {}).get('children', [])
            
            signals = []
            for idx, post in enumerate(posts):
                post_data = post.get('data', {})
                permalink = post_data.get('permalink')
                title = post_data.get('title', 'Untitled')
                selftext = post_data.get('selftext', '')
                author = post_data.get('author', 'unknown')
                subreddit_name = post_data.get('subreddit', subreddit)
                num_comments = post_data.get('num_comments', 0)
                created_utc = post_data.get('created_utc', 0)  # Unix timestamp
                
                # Filter by date if specified
                if before_date and created_utc:
                    post_date = datetime.fromtimestamp(created_utc)
                    filter_date = self._parse_date(before_date)
                    if filter_date and post_date >= filter_date:
                        continue  # Skip posts on or after the filter date
                
                if permalink and title:
                    url = f"https://www.reddit.com{permalink}"
                    
                    # Convert Unix timestamp to ISO format for display
                    publish_date = datetime.fromtimestamp(created_utc).isoformat() if created_utc else None
                    
                    # Fetch comments from the thread
                    print(f"  üì• Fetching comments for: {title[:50]}...")
                    comments = self._fetch_comments_json(permalink, headers, limit=10)
                    
                    # Create combined text for searching
                    combined_text = f"{title} {selftext}"
                    if comments:
                        comments_text = ' '.join([c['text'] for c in comments])
                        combined_text += f" {comments_text}"
                        print(f"    ‚úì Fetched {len(comments)} comments")
                    else:
                        print(f"    ‚ö†Ô∏è No comments fetched")
                    
                    signals.append({
                        'id': f'signal-reddit-{int(time.time())}-{idx}',
                        'source_type': 'social',
                        'source_name': f"Reddit r/{subreddit_name}",
                        'source_url': url,
                        'ingestion_timestamp': datetime.now().isoformat(),
                        'post_title': title,
                        'post_text': selftext,
                        'extracted_text': combined_text[:1000],
                        'comments': comments,
                        'matched_keywords': query.split(' OR '),
                        'inferred_workforce_theme': self._infer_theme(combined_text),
                        'metadata': {
                            'title': title,
                            'author': author,
                            'comment_count': num_comments,
                            'publish_date': publish_date
                        }
                    })
            
            print(f"‚úì Found {len(signals)} Reddit signals via JSON API")
            return signals
            
        except ConnectionError as e:
            # Re-raise connection errors with helpful message
            print(f"‚ùå Reddit connection blocked: {e}")
            print(f"   See REDDIT_TROUBLESHOOTING.md for detailed solutions")
            raise
        except Exception as e:
            error_str = str(e).lower()
            if "10061" in error_str or "refused" in error_str or "connection" in error_str:
                print(f"‚ùå Reddit is blocking connections!")
                print(f"   Error: {e}")
                print(f"   Solutions: Use VPN, different network, or Reddit's official API")
                print(f"   See REDDIT_TROUBLESHOOTING.md for details")
            else:
                print(f"‚ùå JSON API failed: {e}")
            return []
    
    def _fetch_comments_json(
        self,
        permalink: str,
        headers: Dict[str, str],
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Fetch top comments from a Reddit thread using JSON API
        
        Args:
            permalink: Thread permalink (e.g., /r/singapore/comments/...)
            headers: HTTP headers to use
            limit: Maximum number of comments to fetch
            
        Returns:
            List of comment dictionaries with author, text, score
        """
        try:
            # Reddit thread JSON API
            thread_url = f"https://www.reddit.com{permalink}.json"
            
            # Retry logic
            for attempt in range(2):
                try:
                    response = requests.get(thread_url, headers=headers, timeout=10)
                    response.raise_for_status()
                    break
                except (requests.ConnectionError, requests.Timeout):
                    if attempt < 1:
                        time.sleep(2)
                    else:
                        return []
            
            data = response.json()
            
            # Reddit returns [post_data, comments_data]
            if len(data) < 2:
                return []
            
            comments_data = data[1].get('data', {}).get('children', [])
            
            comments = []
            
            for comment in comments_data:
                if len(comments) >= limit:
                    break
                    
                comment_data = comment.get('data', {})
                body = comment_data.get('body', '')
                author = comment_data.get('author', 'unknown')
                score = comment_data.get('score', 0)
                
                # Skip deleted, removed, or AutoModerator comments
                if body and body not in ['[deleted]', '[removed]'] and author != 'AutoModerator':
                    comments.append({
                        'author': author,
                        'text': body,
                        'score': score
                    })
            
            return comments
            
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to fetch comments for {permalink}: {e}")
            return []
    
    def search_workforce_signals(
        self,
        subreddit: str = "singapore",
        keywords: Optional[List[str]] = None,
        before_date: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        Search Reddit for workforce-related discussions
        Uses JSON API (no Selenium/browser needed)
        
        Args:
            subreddit: Subreddit to search
            keywords: Optional keywords to search for
            before_date: Filter posts before this date (YYYY-MM-DD)
            
        Returns:
            List of workforce signals from Reddit
        """
        try:
            # Build search query
            if keywords:
                query = " OR ".join(keywords)
            else:
                query = "workforce layoff job"
            
            # Use JSON API (fast and reliable)
            signals = self.search_using_json_api(subreddit, query, limit=5, before_date=before_date)
            return signals
            
        except Exception as e:
            print(f"Error in search_workforce_signals: {e}")
            return []
    
    def _infer_theme(self, content: str) -> str:
        """Infer workforce theme from content"""
        content_lower = content.lower()
        
        if any(word in content_lower for word in ['retrench', 'layoff', 'lose job', 'fired']):
            return 'Job Loss & Retrenchment'
        elif any(word in content_lower for word in ['hiring', 'job opening', 'career']):
            return 'Job Opportunities'
        elif any(word in content_lower for word in ['salary', 'pay', 'wage', 'compensation']):
            return 'Salary & Compensation'
        elif any(word in content_lower for word in ['work culture', 'toxic', 'burnout']):
            return 'Work Culture & Environment'
        else:
            return 'General Career Discussion'
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        """
        Parse date string to datetime object
        
        Args:
            date_str: Date string in YYYY-MM-DD format
            
        Returns:
            datetime object or None if parsing fails
        """
        if not date_str:
            return None
        
        try:
            return datetime.strptime(date_str, '%Y-%m-%d')
        except ValueError:
            return None

